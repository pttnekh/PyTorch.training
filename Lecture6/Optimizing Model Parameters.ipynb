{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "492a8910",
   "metadata": {},
   "source": [
    "# Optimizing Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bca64e9",
   "metadata": {},
   "source": [
    "지금까지 모델 만들었고, 데이터 불러왔으니 학습을 시켜보자\n",
    "우리 데이터로 파라메터 최적화를 하여 모델을 입증하고 테스트하는 거야\n",
    "모델을 학습하는 건 반복적인 처리인데, 각 반복에서 모델은 출력에 대한 예측을 하고 \n",
    "예측에 대한 오차(loss)를 계산해, 오차를 각 파라미터를 기준 변수로 해서 미분한 값들을 모은다.\n",
    "그리고 이 파라미터들을 경사 하강법 이용하여 최적화한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179426b5",
   "metadata": {},
   "source": [
    "## Prerequisite Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "377a4a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f3ecb1",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed49474",
   "metadata": {},
   "source": [
    "하이퍼파라미터를 조정해서 모델 학습이나 수렴률(얼마나 빨리 수렴에 가까워지는가, 수치해석, 최적화에서 오차가 반복마다 얼마나 빠르게 줄어드는지)에 영향을 줄 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690dcdeb",
   "metadata": {},
   "source": [
    "학습을 위해 정하게 되는 파라미터들\n",
    "- Number of Epochs: 반복횟수\n",
    "- Batch Size: 파라미터 업데이트 전, 한 번에 네트워크에 통과(전파) 시키는 데이터 샘플의 개수\n",
    "- Learning Rate: 각 배치/에폭마다 모델 파라미터를 얼마나 크게 업데이트할지 정하는 값, 값이 작으면 학습이 느려지고, 너무 크면 학습이 불안정해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b30abb",
   "metadata": {},
   "source": [
    "learning_rate = 1e-3 (= 0.001): 업데이트할 때 한 번에 움직이는 **보폭(step size)**이 0.001 정도라는 의미다. 보통 경사하강법 계열 업데이트가 \n",
    "θ\n",
    "←\n",
    "θ\n",
    "−\n",
    "η\n",
    "∇\n",
    "θ\n",
    "L\n",
    "θ←θ−η∇ \n",
    "θ\n",
    " L 형태인데, 여기서 \n",
    "η\n",
    "η가 learning rate야\n",
    "\n",
    "e가 지수승을 나타냄"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6430c67",
   "metadata": {},
   "source": [
    "학습률이 너무 작으면: 업데이트가 너무 조금씩이라 학습이 느리거나 덜 된다.\n",
    "​\n",
    "\n",
    "학습률이 너무 크면: 최솟값 근처에서 튕기거나 발산해서 학습이 불안정해질 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19154463",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edd7969",
   "metadata": {},
   "source": [
    "## Optimization Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62299d60",
   "metadata": {},
   "source": [
    "하이퍼파라미터를 정하면, 우리는 최적화 루프를 가지고 모델을 학습하고 최적화할 수 있다. \n",
    "각 최적화 루프의 반복은 에폭(epoch)라고 부른다.\n",
    "\n",
    "각 에폭은 두가지 부분으로 구성됨\n",
    "- The Train Loop: 학습 데이터셋을 반복하며, 손실을 줄이도록 파라미터를 업데이트해 최적값애 수렴하도록 학습\n",
    "- The Validation/Test Loop: (검증/테스트) 데이터셋을 반복하며, 파라미터 업데이트 없이 성능을 평가해 모델이 개선되고 있는지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb52219",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f0f088",
   "metadata": {},
   "source": [
    "훈련 데이터가 주어졌을 때, 학습되지 않은 네트워크는 정답을 내지 못할 가능성이 크다. 손실 함수는 모델이 얻은 결과가 목표값과 얼마나 다른지의 정도를 측정하며, 학습 과정에서 손실 함수를 최소화하고자 하는 값이 바로 손실이다. 손실을 계산하기 위해, 주어진 데이터 샘플의 입력값으로 예측을 만든 다음 그 예측값을 실제 정답 라벨 값과 비교한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb202f83",
   "metadata": {},
   "source": [
    "기본적인 손실 함수는 회귀(regression) 작업에서 nn.MSELoss(평균제곱오차, Mean Square Error)를, 분류(classification)작업에서는 nn.NLLLose(음의 로그우도, Negative Log Likelihood)를 대표적인 손실 함수로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174349d7",
   "metadata": {},
   "source": [
    "우리 모델의 아웃풋 로짓(각 클래스가 얼마나 그럴듯한지)를 nn.CrossEntrophyLoss에 통과시킴, 그리고 그것은 로짓을 정규화하고 예측 오차를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ebfbd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55b0be4",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55da0e80",
   "metadata": {},
   "source": [
    "모델 오차를 각 트레이닝 단계에서 줄이도록 모델 파라미터를 조정하는 것이 최적화\n",
    "\n",
    "최적화 알고리즘은 어떻게 이 과정이 수행되는지 결정한다.(in this example we use Stochastic Gradient Descent)\n",
    "\n",
    "모든 최적화 로직은 optimizer 객체 않에 캡술화되어있다.\n",
    "\n",
    "SGD optimizer를 사용할건데, 추가로 많은 다른 optimizer들이 있다. (ADAM, RMSProp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52913951",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e68a49",
   "metadata": {},
   "source": [
    "학습 루프에서는 3 단게 동안 최적화가 일어나는데\n",
    "\n",
    "- optimizer.zero_frad() 호출하여 모델 파라미터의 기울기를 초기화, double-counting을 예방, 명시적으로 각 반복에서 경사를 0으로 만듦\n",
    "\n",
    "- loss.backword()를 호출하여 예측 손실을 역전파함, 파이토치가 loss를 각 파라미터에 대해 미분한 gradient를 각 파라미터 객체에 저장해 둔다.\n",
    "\n",
    "- 일단 gradient를 얻고, optimizer.step()을 호출하여 backword pass에서 수집된 gradient에 의해 파라미터를 결정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33555bfc",
   "metadata": {},
   "source": [
    "## Full Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996521a7",
   "metadata": {},
   "source": [
    "train_loop(): 최적화\n",
    "test_loop(): 검증 데이터로 모델 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45499208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4c325a",
   "metadata": {},
   "source": [
    "모델 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5b465de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.313030  [   64/60000]\n",
      "loss: 2.295364  [ 6464/60000]\n",
      "loss: 2.277286  [12864/60000]\n",
      "loss: 2.270979  [19264/60000]\n",
      "loss: 2.256611  [25664/60000]\n",
      "loss: 2.239195  [32064/60000]\n",
      "loss: 2.239461  [38464/60000]\n",
      "loss: 2.210912  [44864/60000]\n",
      "loss: 2.215934  [51264/60000]\n",
      "loss: 2.184968  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 47.2%, Avg loss: 2.180083 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.192493  [   64/60000]\n",
      "loss: 2.180298  [ 6464/60000]\n",
      "loss: 2.129351  [12864/60000]\n",
      "loss: 2.143706  [19264/60000]\n",
      "loss: 2.097535  [25664/60000]\n",
      "loss: 2.048574  [32064/60000]\n",
      "loss: 2.065143  [38464/60000]\n",
      "loss: 1.994414  [44864/60000]\n",
      "loss: 2.009582  [51264/60000]\n",
      "loss: 1.936782  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Avg loss: 1.939402 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.966805  [   64/60000]\n",
      "loss: 1.938055  [ 6464/60000]\n",
      "loss: 1.832011  [12864/60000]\n",
      "loss: 1.873577  [19264/60000]\n",
      "loss: 1.762809  [25664/60000]\n",
      "loss: 1.712655  [32064/60000]\n",
      "loss: 1.730412  [38464/60000]\n",
      "loss: 1.630025  [44864/60000]\n",
      "loss: 1.663462  [51264/60000]\n",
      "loss: 1.553869  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Avg loss: 1.575612 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.630195  [   64/60000]\n",
      "loss: 1.595494  [ 6464/60000]\n",
      "loss: 1.455659  [12864/60000]\n",
      "loss: 1.527630  [19264/60000]\n",
      "loss: 1.403004  [25664/60000]\n",
      "loss: 1.398975  [32064/60000]\n",
      "loss: 1.400993  [38464/60000]\n",
      "loss: 1.323991  [44864/60000]\n",
      "loss: 1.366102  [51264/60000]\n",
      "loss: 1.264191  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 1.291976 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.358508  [   64/60000]\n",
      "loss: 1.340830  [ 6464/60000]\n",
      "loss: 1.183560  [12864/60000]\n",
      "loss: 1.289607  [19264/60000]\n",
      "loss: 1.163451  [25664/60000]\n",
      "loss: 1.190410  [32064/60000]\n",
      "loss: 1.195212  [38464/60000]\n",
      "loss: 1.130071  [44864/60000]\n",
      "loss: 1.180311  [51264/60000]\n",
      "loss: 1.096806  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.116377 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.178073  [   64/60000]\n",
      "loss: 1.180649  [ 6464/60000]\n",
      "loss: 1.006492  [12864/60000]\n",
      "loss: 1.143804  [19264/60000]\n",
      "loss: 1.016330  [25664/60000]\n",
      "loss: 1.051574  [32064/60000]\n",
      "loss: 1.070379  [38464/60000]\n",
      "loss: 1.006989  [44864/60000]\n",
      "loss: 1.059546  [51264/60000]\n",
      "loss: 0.993748  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 65.6%, Avg loss: 1.003728 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.053679  [   64/60000]\n",
      "loss: 1.078128  [ 6464/60000]\n",
      "loss: 0.886258  [12864/60000]\n",
      "loss: 1.047548  [19264/60000]\n",
      "loss: 0.923443  [25664/60000]\n",
      "loss: 0.954061  [32064/60000]\n",
      "loss: 0.989611  [38464/60000]\n",
      "loss: 0.926730  [44864/60000]\n",
      "loss: 0.974907  [51264/60000]\n",
      "loss: 0.925162  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 67.0%, Avg loss: 0.926751 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.962229  [   64/60000]\n",
      "loss: 1.007731  [ 6464/60000]\n",
      "loss: 0.800148  [12864/60000]\n",
      "loss: 0.979408  [19264/60000]\n",
      "loss: 0.861307  [25664/60000]\n",
      "loss: 0.882464  [32064/60000]\n",
      "loss: 0.932923  [38464/60000]\n",
      "loss: 0.872638  [44864/60000]\n",
      "loss: 0.912927  [51264/60000]\n",
      "loss: 0.875437  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 68.2%, Avg loss: 0.870741 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.891571  [   64/60000]\n",
      "loss: 0.955246  [ 6464/60000]\n",
      "loss: 0.735360  [12864/60000]\n",
      "loss: 0.928111  [19264/60000]\n",
      "loss: 0.816308  [25664/60000]\n",
      "loss: 0.827733  [32064/60000]\n",
      "loss: 0.889830  [38464/60000]\n",
      "loss: 0.834472  [44864/60000]\n",
      "loss: 0.865907  [51264/60000]\n",
      "loss: 0.837061  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 69.7%, Avg loss: 0.827852 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.835075  [   64/60000]\n",
      "loss: 0.913279  [ 6464/60000]\n",
      "loss: 0.684465  [12864/60000]\n",
      "loss: 0.888052  [19264/60000]\n",
      "loss: 0.781298  [25664/60000]\n",
      "loss: 0.784921  [32064/60000]\n",
      "loss: 0.854750  [38464/60000]\n",
      "loss: 0.806171  [44864/60000]\n",
      "loss: 0.828964  [51264/60000]\n",
      "loss: 0.805911  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 71.1%, Avg loss: 0.793491 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78876ec2",
   "metadata": {},
   "source": [
    "에폭을 거듭할수록 정확도가 상승하고 평균 loss가 하강 -> 학습이 진행되며 성능이 개선되고 있음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
